{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SAV_smn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sungmino/TestGit/blob/master/SAV_smn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVipyfmQiZF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec20047-2fba-40a3-b198-5ca347c7abb6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p65nosKTA30c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCRljPJrBPVf",
        "outputId": "d99b8515-c00f-422e-b6df-6f94cf59c68d"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnESdpFPBP8H"
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IOf9mAQBQHO"
      },
      "source": [
        "!pip install fairseq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJAwg4siBQQm"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAgvSQ1qBQZb"
      },
      "source": [
        "!pip install fastBPE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0oDPUYbBQ30"
      },
      "source": [
        "!pip install vncorenlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bftuKJvHBRAO"
      },
      "source": [
        "# ---------- Train model ----------------------\n",
        "\n",
        "from vncorenlp import VnCoreNLP\n",
        "rdrsegmenter = VnCoreNLP(\"/content/drive/MyDrive/SAV_Project/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.data import Dictionary\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--bpe-codes',\n",
        "                        default=\"/content/drive/MyDrive/SAV_Project/PhoBERT_base_transformers/bpe.codes\",\n",
        "                        required=False,\n",
        "                        type=str,\n",
        "                        help='path to fastBPE BPE'\n",
        "                        )\n",
        "args, unknown = parser.parse_known_args()\n",
        "bpe = fastBPE(args)\n",
        "    # Load the dictionary\n",
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"/content/drive/MyDrive/SAV_Project/PhoBERT_base_transformers/dict.txt\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "633dWS6wBeD6",
        "outputId": "2b7eb7de-f8fb-4144-b68b-0879677c3226"
      },
      "source": [
        "bpe.encode(\"Hôm_nay trời nóng quá nên tôi ở nhà viết Viblo!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Hôm_nay trời nóng quá nên tôi ở nhà viết Vi@@ blo@@ !'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REEbSxewBeZf"
      },
      "source": [
        "import re\n",
        "import json\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "train_path = '/content/drive/MyDrive/SAV_Project/data/train/train.json'\n",
        "test_path = '/content/drive/MyDrive/SAV_Project/data/test/test.json'\n",
        "\n",
        "def load_data_format(filename1):\n",
        "    with open(filename1, 'r') as file:\n",
        "        contents = json.load(file)\n",
        "        file.close()\n",
        "    return contents\n",
        "\n",
        "train_id, train_text, train_label = [], [], []\n",
        "test_id, test_text = [], []\n",
        "\n",
        "train_data = load_data_format(train_path)\n",
        "train_data = train_data[\"reviews\"]\n",
        "for k in tqdm_notebook(train_data):\n",
        "    train_id.append(k[\"id\"])\n",
        "    text = k[\"comment\"]\n",
        "    text = rdrsegmenter.tokenize(text)\n",
        "    text = ' '.join([' '.join(x) for x in text])\n",
        "    train_text.append(text)\n",
        "    train_label.append(k[\"label\"])\n",
        "\n",
        "test_data = load_data_format(test_path)\n",
        "test_data = test_data[\"reviews\"]\n",
        "for n in tqdm_notebook(test_data):\n",
        "    test_id.append(n[\"id\"])\n",
        "    texts = n[\"comment\"]\n",
        "    texts = rdrsegmenter.tokenize(texts)\n",
        "    texts = ' '.join([' '.join(x) for x in texts])\n",
        "    test_text.append(texts)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i6EnrowFrJK"
      },
      "source": [
        "train_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArIzx2W8HIEl"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sents, val_sents, train_labels, val_labels = train_test_split(train_text, train_label, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8PcvD0VHhYF"
      },
      "source": [
        "val_sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY7Q8rtHG2ps"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 150\n",
        "train_ids = []\n",
        "for tx in train_sents:\n",
        "    subwords = '<s> ' + bpe.encode(tx) + ' </s>'\n",
        "    encoded_tx = vocab.encode_line(subwords, append_eos=True, add_if_not_exist=False).long().tolist()\n",
        "    train_ids.append(encoded_tx)\n",
        "val_ids = []\n",
        "for tx in val_sents:\n",
        "    subwords = '<s> ' + bpe.encode(tx) + ' </s>'\n",
        "    encoded_tx = vocab.encode_line(subwords, append_eos=True, add_if_not_exist=False).long().tolist()\n",
        "    val_ids.append(encoded_tx)\n",
        "\n",
        "# tao list id của các subword có trong từ điển\n",
        "train_ids = pad_sequences(train_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "val_ids = pad_sequences(val_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BnpGyTwKFYR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIAANEr4IfFI"
      },
      "source": [
        " # tao mask, mask cho biet cac value nao da duoc padding\n",
        "train_masks = []\n",
        "for tx in train_ids:\n",
        "    mask = [int(token_id > 0) for token_id in tx]\n",
        "    train_masks.append(mask)\n",
        "val_masks = []\n",
        "for tx in val_ids:\n",
        "    mask = [int(token_id > 0) for token_id in tx]\n",
        "    val_masks.append(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Up-S6gxI2BG"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "\n",
        "# tao dataloader\n",
        "train_inputs = torch.tensor(train_ids)\n",
        "val_inputs = torch.tensor(val_ids)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "val_labels = torch.tensor(val_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = SequentialSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2FEWgXDJaDt",
        "outputId": "76cffa34-77ad-49c5-f79b-e16dd6db5a02"
      },
      "source": [
        "train_dataloader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f3b352664d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BhgIZL-Jdb1"
      },
      "source": [
        "from transformers import RobertaForSequenceClassification, RobertaConfig, AdamW\n",
        "\n",
        "config = RobertaConfig.from_pretrained(\n",
        "        \"/content/drive/MyDrive/SAV_Project/PhoBERT_base_transformers/config.json\", from_tf=False, num_labels=2, output_hidden_states=False,\n",
        "    )\n",
        "\n",
        "BERT_SA = RobertaForSequenceClassification.from_pretrained(\n",
        "      \"/content/drive/MyDrive/SAV_Project/PhoBERT_base_transformers/model.bin\",\n",
        "      config=config\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esnUAz0sKAN3"
      },
      "source": [
        "BERT_SA.cuda()\n",
        "print('OK DONE...')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQgLgEKoKHSW"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    \n",
        "    F1_score = f1_score(pred_flat, labels_flat, average='macro')\n",
        "    \n",
        "    return accuracy_score(pred_flat, labels_flat), F1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo2k48XkKR6f"
      },
      "source": [
        "import random\n",
        "from tqdm import tqdm_notebook\n",
        "device = 'cuda'\n",
        "epochs = 4\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "param_optimizer = list(BERT_SA.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5, correct_bias=False)\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    total_loss = 0\n",
        "    BERT_SA.train()\n",
        "    train_accuracy = 0\n",
        "    nb_train_steps = 0\n",
        "    train_f1 = 0\n",
        "    if torch.cuda.is_available():\n",
        "          BERT_SA.to(device)\n",
        "    for step, batch in tqdm_notebook(enumerate(train_dataloader)):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        BERT_SA.zero_grad()\n",
        "        outputs = BERT_SA(b_input_ids, \n",
        "            token_type_ids=None, \n",
        "            attention_mask=b_input_mask, \n",
        "            labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        tmp_train_accuracy, tmp_train_f1 = flat_accuracy(logits, label_ids)\n",
        "        train_accuracy += tmp_train_accuracy\n",
        "        train_f1 += tmp_train_f1\n",
        "        nb_train_steps += 1\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(BERT_SA.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\" Accuracy: {0:.4f}\".format(train_accuracy/nb_train_steps))\n",
        "    print(\" F1 score: {0:.4f}\".format(train_f1/nb_train_steps))\n",
        "    print(\" Average training loss: {0:.4f}\".format(avg_train_loss))\n",
        "    train_acc = \"{0:.4f}\".format(train_accuracy/nb_train_steps)\n",
        "    train_f1c = \"{0:.4f}\".format(train_f1/nb_train_steps)\n",
        "    train_loss = \"{0:.4f}\".format(avg_train_loss)\n",
        "\n",
        "    print(\"Running Validation...\")\n",
        "    BERT_SA.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    eval_f1 = 0\n",
        "    for batch in tqdm_notebook(val_dataloader):\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = BERT_SA(b_input_ids, \n",
        "            token_type_ids=None, \n",
        "            attention_mask=b_input_mask)\n",
        "            logits = outputs[0]\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            tmp_eval_accuracy, tmp_eval_f1 = flat_accuracy(logits, label_ids)\n",
        "\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "            eval_f1 += tmp_eval_f1\n",
        "            nb_eval_steps += 1\n",
        "    print(\" Accuracy: {0:.4f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\" F1 score: {0:.4f}\".format(eval_f1/nb_eval_steps))\n",
        "\n",
        "    val_acc = \"{0:.4f}\".format(eval_accuracy/nb_eval_steps)\n",
        "    val_f1c = \"{0:.4f}\".format(eval_f1/nb_eval_steps)\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_f1'].append(train_f1c)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_f1'].append(val_f1c)\n",
        "\n",
        "    if (float(val_acc) > best_accuracy):\n",
        "      torch.save(BERT_SA.state_dict(), '/content/drive/MyDrive/SAV_Project/best_model_state.bin')\n",
        "      best_accuracy = val_acc\n",
        "print(\"Training complete!\") \n",
        "\n",
        "# ------------------ End train model ------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlOU2zDut4Zj",
        "outputId": "1670b579-54cc-4a57-c417-431c999e0278"
      },
      "source": [
        "# ---------------------- Predict text------------------------\n",
        "\n",
        "from vncorenlp import VnCoreNLP\n",
        "rdrsegmenter = VnCoreNLP(\"/content/drive/MyDrive/SAV_Project/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "from fairseq.data import Dictionary\n",
        "import argparse\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "from transformers import RobertaForSequenceClassification, RobertaConfig, AdamW\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from collections import defaultdict\n",
        "from string import punctuation\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--bpe-codes',\n",
        "                        default=\"/content/drive/MyDrive/SAV_Project/PhoBERT_base_transformers/bpe.codes\",\n",
        "                        required=False,\n",
        "                        type=str,\n",
        "                        help='path to fastBPE BPE'\n",
        "                        )\n",
        "args, unknown = parser.parse_known_args()\n",
        "bpe = fastBPE(args)\n",
        "    # Load the dictionary\n",
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"/content/drive/MyDrive/SAV_Project/PhoBERT_base_transformers/dict.txt\")\n",
        "config = RobertaConfig.from_pretrained(\n",
        "        \"/content/drive/MyDrive/SAV_Project/PhoBERT_base_transformers/config.json\", from_tf=False, num_labels=2, output_hidden_states=False,\n",
        "    )\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "      \"/content/drive/MyDrive/SAV_Project/best_model_state.bin\",\n",
        "      config=config\n",
        "    )\n",
        "device = 'cuda'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo-YZWlUMz9n",
        "outputId": "0bdb4cbe-e6f8-4512-fca6-d748fba2d4cf"
      },
      "source": [
        "def remove_punctuation(list_string):\n",
        "    \"\"\"xóa các punctuation ra khỏi từ và câu\"\"\"\n",
        "    # nếu là ký tự đặc biệt thì xóa\n",
        "    for token in list_string.split(' '):\n",
        "        if token in punctuation:\n",
        "            list_string = list_string.replace(token, '')\n",
        "        return list_string\n",
        "\n",
        "def removie_Special_characters(list_string):\n",
        "    string_check = re.compile('[@_!#$%^&*()<>?/\\|¥+}{~:;]')\n",
        "    list_string = re.sub(string_check, ' ', list_string)\n",
        "    return list_string\n",
        "\n",
        "def normalizeString(list_strings):\n",
        "    \"\"\"Tách dấu ra khỏi từ\"\"\"\n",
        "    str = list_strings.lower()\n",
        "    # Tách dấu câu nếu kí tự liền nhau\n",
        "    marks = '[.!?,-${}()]'\n",
        "    r = \"([\" + \"\\\\\".join(marks) + \"])\"\n",
        "    str = re.sub(r, r\" \\1 \", str)\n",
        "    # Thay thế nhiều spaces bằng 1 space.\n",
        "    str = re.sub(r\"\\s+\", r\" \", str).strip()\n",
        "    return str\n",
        "\n",
        "def strip_emoji(text):\n",
        "    \"\"\"Xóa icons\"\"\"\n",
        "    RE_EMOJI = re.compile(\n",
        "        u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])|([\\U0001F1E0-\\U0001F1FF])|([\\U0001F300-\\U0001F5FF])|([\\U0001F600-\\U0001F64F])|([\\U0001F680-\\U0001F6FF])|([\\U0001F700-\\U0001F77F])|([\\U0001F780-\\U0001F7FF])|([\\U0001F800-\\U0001F8FF])|([\\U0001F900-\\U0001F9FF])|([\\U0001FA00-\\U0001FA6F])|([\\U0001FA70-\\U0001FAFF])|([\\U00002702-\\U000027B0])')\n",
        "    return RE_EMOJI.sub(r'', text)\n",
        "\n",
        "def remove_duplicate_characters(list_string):\n",
        "    \"\"\"Xóa các ký tự kéo dài: vd: đẹppppppp\"\"\"\n",
        "    list_string = re.sub(r'([A-Z])\\1+', lambda m: m.group(1), list_string, flags=re.IGNORECASE)\n",
        "    return list_string\n",
        "\n",
        "def remove_char_spams(list_string):\n",
        "        for token in list_string.split(' '):\n",
        "            if (len(token) > 8):\n",
        "                list_string = list_string.replace(token, '')\n",
        "        return list_string\n",
        "\n",
        "def replace_abbreviations(text):\n",
        "        \"\"\"Chuẩn hóa từ\"\"\"\n",
        "        text = remove_duplicate_characters(text)\n",
        "        replace_list = {\n",
        "            ' ô kêi ': ' ok ', ' okie ': ' ok ', ' o kê ': ' ok ',\n",
        "            ' okey ': ' ok ', ' ôkê ': ' ok ', ' oki ': ' ok ', ' oke ': ' ok ', ' okay ': ' ok ', ' okê ': ' ok ',\n",
        "            ' tks ': u' cám ơn ', ' thks ': u' cám ơn ', ' thanks ': u' cám ơn ', ' ths ': u' cám ơn ',\n",
        "            ' thank ': u' cám ơn ',\n",
        "            ' not ': u' không ', u' kg ': u' không ', ' kh ': u' không ', ' kp ': u' không phải ', u' kô ': u' không ',\n",
        "            '\"ko ': u' không ', u' ko ': u' không ', u' k ': u' không ', ' khong ': u' không ', u' hok ': u' không ',\n",
        "            ' he he ': ' positive ', ' hehe ': ' positive ', ' hihi ': ' positive ', ' haha ': ' positive ',\n",
        "            ' hjhj ': ' positive ',\n",
        "            ' lol ': ' nagative ', ' cc ': ' nagative ', ' cute ': u' dễ thương ', ' huhu ': ' nagative ', ' vs ': u' với ',\n",
        "            ' wa ': ' quá ', ' wá ': u' quá ', ' j ': u' gì ', '“': ' ',\n",
        "            ' sz ': u' cỡ ', ' size ': u' cỡ ', u' đx ': u' được ', ' dc ': u' được ', ' đk ': u' được ',\n",
        "            ' đc ': u' được ', ' authentic ': u' chuẩn chính hãng ', u' aut ': u' chuẩn chính hãng ',\n",
        "            u' auth ': u' chuẩn chính hãng ', ' thick ': u' positive ', ' store ': u' cửa hàng ',\n",
        "            ' shop ': u' cửa hàng ', 'sp': u' sản phẩm ', 'gud': u' tốt ', ' god ': u' tốt ', ' wel done ': ' tốt ',\n",
        "            ' good ': u' tốt ', ' gút ': u' tốt ',\n",
        "            ' sấu ': u' xấu ', ' gut ': u' tốt ', u' tot ': u' tốt ', u' nice ': u' tốt ', ' perfect ': ' rất tốt ',\n",
        "            ' bt ': u' bình thường ', ' bthg ': u' bình thường ', ' thg ': u' thường ',\n",
        "            ' time ': u' thời gian ', 'qá': u' quá ', u' ship ': u' giao ', u' m ': u' mình ', u' mik ': u' mình ',\n",
        "            ' ê ̉': 'ể', 'product': 'sản phẩm', ' quality ': ' chất lượng ', ' chat ': ' chất ', ' excelent ': ' hoàn hảo ',\n",
        "            ' bad ': ' tệ ', ' fresh ': ' tươi ', ' sad ': ' tệ ',\n",
        "            ' date ': u' hạn sử dụng ', ' hsd ': u' hạn sử dụng ', ' quickly ': u' nhanh ', ' quick ': u' nhanh ',\n",
        "            ' fast ': u' nhanh ', ' delivery ': u' giao hàng ', u' síp ': u' giao hàng ', ' shiper ': u' người giao hàng ',\n",
        "            ' beautiful ': u' đẹp tuyệt vời ', u' tl ': u' trả lời ', u' r ': u' rồi ', u' shopE ': u' cửa hàng ',\n",
        "            u' order ': u' đặt hàng ',\n",
        "            ' chất lg ': u' chất lượng ', u' sd ': u' sử dụng ', u' dt ': u' điện thoại ', u' nt ': u' nhắn tin ',\n",
        "            u' sài ': u' xài ', u' bjo ': u' bao giờ ',\n",
        "            ' thik ': u' thích ', u' sop ': u' cửa hàng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rất ',\n",
        "            u' quả ng ': u' quảng ', ' tc ': u' tính chất ',\n",
        "            ' dep ': u' đẹp ', u' xau ': u' xấu ', ' delicious ': u' ngon ', u' hàg ': u' hàng ', u' qủa ': u' quả ',\n",
        "            ' iu ': u' yêu ', ' fake ': u' giả mạo ', ' trl ': ' trả lời ', ' >< ': u' positive ',\n",
        "            ' por ': u' tệ ', ' poor ': u' tệ ', ' ib ': u' nhắn tin ', ' rep ': u' trả lời ', u' fback ': ' feedback ',\n",
        "            ' fedback ': ' feedback ', ' tg ': u' thời gian ', ' sp ': u' sản phẩm ', ' cg ': u' cũng ',\n",
        "            ' mk ': u' mình ', ' vs ': u' với ', ' qc ': u' quảng cáo ', ' mng ': u' mọi người ', ' mn ': u' mọi người ',\n",
        "            ' kb ': u' không biết ', ' e ': u' em ', ' ak ': u' à ', ' bh ': u' bao giờ ', ' bn ': u' bao nhiêu ',\n",
        "            ' cỡ mình ': u' cỡ M ', ' ntn ': u' như thế nào ', ' z ': u' vậy ', u' nhìu ': u' nhiều ', ' ah ': u' à ',\n",
        "            ' vlin ': ' negative ', ' vl ': ' negative ', ' vch ': ' negative ', ' vcl ': ' negative ', ' cmt ': u' bình luận '\n",
        "        }\n",
        "\n",
        "        for k, v in replace_list.items():\n",
        "            text = text.replace(k, v)\n",
        "            # xóa số  ra khỏi chuỗi\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "\n",
        "def normalize_text(list_string):\n",
        "    list_string = list_string.lower().replace('\\n', ' ')\n",
        "    list_string = removie_Special_characters(list_string)\n",
        "    list_string = strip_emoji(list_string)\n",
        "    list_string = remove_duplicate_characters(list_string)\n",
        "    list_string = remove_char_spams(list_string)\n",
        "    list_string = replace_abbreviations(list_string)\n",
        "    list_string = remove_punctuation(list_string)\n",
        "    list_string = normalizeString(list_string)\n",
        "    return list_string\n",
        "\n",
        "\n",
        "def prepare_text(list_string = [], *args):\n",
        "  MAX_LEN = 150\n",
        "\n",
        "  text_ids = []\n",
        "  for tx in list_string:\n",
        "    tx = normalize_text(tx)\n",
        "    text_tokenizer = rdrsegmenter.tokenize(tx)\n",
        "    texts_tk = ' '.join([' '.join(x) for x in text_tokenizer])\n",
        "    subwords = '<s> ' + bpe.encode(texts_tk) + ' </s>'\n",
        "    encoded_tx = vocab.encode_line(subwords, append_eos=True, add_if_not_exist=False).long().tolist()\n",
        "\n",
        "    text_ids.append(encoded_tx)\n",
        "\n",
        "  text_ids = pad_sequences(text_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  text_masks = []\n",
        "  for tx in text_ids:\n",
        "    mask = [int(token_id > 0) for token_id in tx]\n",
        "    text_masks.append(mask)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    model.to(device)\n",
        "  input_ids = torch.tensor(text_ids).to(device)\n",
        "  input_masks = torch.tensor(text_masks).to(device)\n",
        "\n",
        "  return input_ids, input_masks\n",
        "\n",
        "def predict_text(list_string = [], *args):\n",
        "  start_time = time.time()\n",
        "  input_ids, input_masks = prepare_text(list_string)\n",
        "  output = model(input_ids, token_type_ids=None, attention_mask = input_masks)\n",
        "  print(output)\n",
        "\n",
        "  print(f'Review text: {review_text}')\n",
        "  logits = output[0]\n",
        "  probs = torch.softmax(logits, dim=1).detach().cpu().numpy()\n",
        "  end_time = time.time()\n",
        "  print(probs)\n",
        "  time_pre = end_time - start_time\n",
        "  print('Duration: {}'.format(end_time - start_time))\n",
        "  return probs, time_pre\n",
        "\n",
        "\n",
        "review_text = [\"sản phẩm dùng tốt nha mọi người\", \"sản phẩm xấu và giao hàng chậm\", \"hàng đẹp nhưng dễ vỡ, giao hàng chậm\"]\n",
        "pre, time_pre = predict_text(review_text)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3141, -2.5678],\n",
            "        [-2.6853,  2.7849],\n",
            "        [-2.0787,  2.1960]], device='cuda:0', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
            "Review text: ['sản phẩm dùng tốt nha mọi người', 'sản phẩm xấu và giao hàng chậm', 'hàng đẹp nhưng dễ vỡ, giao hàng chậm']\n",
            "[[0.992475   0.00752502]\n",
            " [0.0041927  0.9958073 ]\n",
            " [0.01372508 0.98627496]]\n",
            "Duration: 0.05650162696838379\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2KPX-S7HC5F"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdz-JUTQHKWg"
      },
      "source": [
        "from flask import *\n",
        "from flask_ngrok import run_with_ngrok\n",
        "import itertools\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "@app.route('/')\n",
        "def home():\n",
        "  return 'Hello World'\n",
        "\n",
        "@app.route('/predict', methods=['POST', 'GET'])\n",
        "def predict():\n",
        "  if (request.method == 'POST'):\n",
        "    output = []\n",
        "    data = request.form['data']\n",
        "    data_pred, time_pred = predict_text(data)\n",
        "    for (score, tx) in zip(data_pred, data):\n",
        "      if (score[0] > score[1]):\n",
        "        pre_db = {\n",
        "              \"comment\": tx,\n",
        "              \"label\": 0\n",
        "          }\n",
        "        output.append(pre_db)\n",
        "      else:\n",
        "          pre_db = {\n",
        "              \"comment\": tx,\n",
        "              \"label\": 1\n",
        "        }\n",
        "      output.append(pre_db)\n",
        "    db_pred = {\"time_pred\": time_pred, \"reviews\": output}\n",
        "    return jsonify(db_pred)\n",
        "  else:\n",
        "      return jsonify({'message': 'Vui lòng chọn phương thức post...Amen....'})\n",
        "\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3rS1MYHHQd7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}